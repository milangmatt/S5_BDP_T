# History of Big Data and Hadoop
=====================



## Introduction

Big data has become an integral part of our lives, transforming the way we live, work, and interact with each other. But have you ever wondered how it all began? In this brief history, we'll take you on a journey through the evolution of big data, from its humble beginnings to its current state and future directions.

## Early Beginnings

The concept of big data dates back to the 1960s, when the US Department of Defense's Advanced Research Projects Agency (ARPA) funded a project to create a network of computers that could communicate with each other. This project, called ARPANET, was the precursor to the modern-day internet.

In the 1980s, the term "big data" was first coined by **John Mashey**, a computer scientist who worked at Silicon Graphics. Mashey used the term to describe the large amounts of data being generated by the company's computer systems.

## The Rise of Big Data

The rise of big data can be attributed to several factors, including:

* The widespread adoption of the internet and mobile devices
* The proliferation of social media platforms
* The increasing use of sensors and IoT devices
* The development of new technologies, such as Hadoop and NoSQL databases

These factors have led to an exponential growth in the amount of data being generated, stored, and analyzed. Today, big data is used in a wide range of applications, from business intelligence and marketing to healthcare and finance.

## The Era of *Big Data*
We are undoubtedly living in the era of "Big Data." This phrase has become a common expression, and its significance is evident in today's technology-driven world. With substantial advancements in computing power, increased accessibility to the internet, and widespread use of electronic devices, the ability to transmit and collect data has reached unprecedented levels.

Key indicators of this trend include:
* Organizations generating data at an incredible pace.
* Facebook alone collecting 250 terabytes of data daily.
* According to **Thompson Reuters News Analytics**, digital data production has more than doubled from approximately 1 million **petabytes** (equivalent to about 1 billion **terabytes**) in 2009 to a projected 7.9 **zettabytes** (with 1 **zettabyte** equal to 1 million **petabytes**) in 2015, and an estimated 35 **zettabytes** in 2020.
* Other research organizations have reported even higher estimates.

As organizations have begun to collect and produce massive amounts of data, they have come to realize the benefits of data analysis. However, they have also encountered difficulties in managing the vast amounts of information at their disposal. This has given rise to several challenges:

* What is the most effective way to store large quantities of data?
* How can you efficiently process this data?
* What methods can be employed to analyze data in a timely manner?
* Given that data will continue to grow, how can you develop a scalable solution?


## History of Hadoop


### Timeline

* 1999: Doug Cutting creates Apache Lucene
* 2002: Doug Cutting and Mike Cafarella start working on Apache Nutch project
* 2003: Google publishes paper on Google File System (GFS)
* 2004: Google publishes paper on MapReduce
* 2005: Doug Cutting joins Yahoo!
* 2006: Hadoop project is formed
* 2007: Yahoo! tests Hadoop on 1000 node cluster
* 2008: Hadoop is released as open source project to Apache Software Foundation
* 2009: Hadoop sorts 1 PB of data in less than 17 hours
* 2011: Apache Hadoop version 1.0 is released
* 2013: Apache Hadoop version 2.0.6 is released
* 2017: Apache Hadoop version 3.0 is released

### Descriptive Timeline

#### 1999: The Beginning of Open-Source Contributions

Doug Cutting creates Apache Lucene, a free and open-source information retrieval software library. This experience will later influence his decision to make Hadoop an open-source project.

#### 2002: The Apache Nutch Project

Doug Cutting and Mike Cafarella start working on the Apache Nutch project, aiming to build a search engine system that can index 1 billion pages. However, they soon realize that the project's architecture is not scalable enough to handle billions of pages on the web.

#### 2003: Google File System (GFS)

Google publishes a paper on the Google File System (GFS), a distributed file system designed to store large amounts of data. Doug Cutting and Mike Cafarella come across this paper and realize that it can solve their problem of storing large files generated by web crawling and indexing processes.

#### 2004: MapReduce

Google publishes another paper on the MapReduce technique, which provides a solution for processing large datasets. This paper is another crucial piece of the puzzle for Doug Cutting and Mike Cafarella.

#### 2005: Joining Yahoo!

Doug Cutting joins Yahoo!, which has a large team of engineers eager to work on the Nutch project. However, he soon realizes that the project is limited to only 20-40 node clusters and that it won't achieve its potential until it runs reliably on larger clusters.

#### 2006: The Birth of Hadoop

Doug Cutting separates the distributed computing parts from Nutch and forms a new project called Hadoop. He wants to create an open-source, reliable, scalable computing framework that can work well on thousands of nodes.

#### 2007: Yahoo! Tests Hadoop

Yahoo! successfully tests Hadoop on a 1000 node cluster and starts using it.

#### 2008: Hadoop Goes Open Source

In January, Yahoo! releases Hadoop as an open source project to the Apache Software Foundation. In July, the Apache Software Foundation successfully tests a 4000 node cluster with Hadoop.

#### 2009: Hadoop Sorts 1 PB of Data

Hadoop is successfully tested to sort 1 PB of data in less than 17 hours, demonstrating its ability to handle large-scale data processing. Doug Cutting leaves Yahoo! and joins Cloudera to spread Hadoop to other industries.

#### 2011: Apache Hadoop Version 1.0

The Apache Software Foundation releases Apache Hadoop version 1.0.

#### 2013: Apache Hadoop Version 2.0.6

Apache Hadoop version 2.0.6 is released.

#### 2017: Apache Hadoop Version 3.0

Apache Hadoop version 3.0 is released, marking a significant milestone in the project's evolution.

In the past 10 years, Hadoop has evolved from its search engine-related origins to one of the most popular general-purpose computing platforms for solving Big Data challenges. It is quickly becoming the foundation for the next generation of data-based applications.


## Conclusion

In conclusion, the history of big data and Hadoop is a rich and fascinating one, marked by significant milestones and innovations. From its humble beginnings in the 1960s to the present day, big data has evolved to become an integral part of our lives, transforming the way we live, work, and interact with each other. The development of Hadoop, with its ability to store, process, and analyze large amounts of data, has been a crucial factor in this evolution. As we look to the future, it is clear that big data and Hadoop will continue to play a vital role in shaping the world around us.